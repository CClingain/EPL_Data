---
title: "K-means Clustering of Premier League Teams 16/17"
author: "Clare Clingain"
date: "May 1, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
require(mclust)
library(NbClust)
require(phyclust)
library(cluster)
library(rgl)
EPL_aggregate <- read.csv("Clean Data/EPL_aggregate.csv")
```

# Introduction

Sports analysts and fans alike try to decipher what sets England's top teams apart from each other. What makes the elusive top 4/6 so far away from the rest? What about the relegation-threatened teams? Using k-means unsupervised learning, we will attempt to cluster teams based on season-total statistics from the 2016-2017 season to determine whether there are ways in which we can separate the top-performing teams from the rest of the pack. Similarly, we will try to see which teams are squeezing their way into the top 4/6. 

# Data Preparation

Using publicly available data we cleaned and manipulated, we created a new data set *EPL_Aggregate*, which can be found [here](https://github.com/CClingain/EPL_Data/tree/master/Clean%20Data), that contains the summary statistics for the 20 Premier League teams from 2000/2001 to 2017/2018 seasons. Twelve different stats were recorded over the course of each season for each team: Yellow Cards, Red Cards, Goals Scored, Goals Against, Fouls Committed, Fouls Against, Corners Won, Corners Conceded, Shots, Shots Conceded, Shots on Target, Shots on Target Conceded. For this analysis, we summed each of the twelve stats for each team to get an idea of the "total" performance of a team over the course of the  2016/2017 season.

**Teams for 16/17:** Arsenal, Bournemouth, Burnley, Chelsea, Crystal Palace, Everton, Hull, Leicester City, Liverpool, Manchester City, Manchester United, Middlesbrough, Southampton, Stoke, Sunderland, Swansea, Tottenham, Watford, West Bromwich Albion, West Ham United.

```{r Data Preparation}
EPL_Cluster <- EPL_aggregate %>% filter(Season=="16/17") %>% group_by(Team) %>%
  mutate(YellowCardSum = sum(Yellow_Cards))

EPL_Cluster <- EPL_Cluster %>% mutate(RedCardSum = sum(Red_Cards)) %>% 
  mutate(GoalsSum = sum(Goals_Scored)) %>%
  mutate(GoalsAgainstSum = sum(Goals_Conceded)) %>% 
  mutate(FoulsSum = sum(Fouls_Committed)) %>% 
  mutate(FoulsAgainstSum = sum(Fouls_Against)) %>%
  mutate(CornersSum = sum(Corners)) %>% 
  mutate(CornersConcededSum = sum(Corners_Conceded)) %>% 
  mutate(ShotsSum = sum(Shots_Taken)) %>%
  mutate(ShotsTargetSum = sum(Shots_on_Target)) %>%
  mutate(ShotsCondededSum = sum(Shots_Conceded)) %>% 
  mutate(ShotsTargetConSum = sum(Shots_on_Target_Conceded))
EPL_Cluster <- EPL_Cluster %>% filter(Week==38)
EPL_Cluster <- EPL_Cluster[c(1:4, 31:42)]
```

# How many clusters?

There are a couple packages out there that do a good job of determining the number of clusters that might exist in a given data set. The Mclust package provides BIC, classification, uncertainty, and density for model-based clustering using an EM algorithm. 

```{r clusters suggestion}
mcl <- Mclust(EPL_Cluster[c(5:16)])
summary(mcl)
```

The results from Mclust reveal that the best model for our data is a 2-cluster VEV solution. The suggested clustering split has 15 teams in Cluster 1 and 5 teams in Cluster 2. However, this portion of the results from Mclust should be taken with caution. K-means may very well cluster teams in a different way.

```{r NbClust}
k.means <- NbClust(EPL_Cluster[c(5:16)],method='kmeans',index='ch')
k.means$Best.nc
table(k.means$Best.partition)
```

NbClust provides a similar analysis for determining the optimal number of clusters in a data set. The maximum index is selected. We chose to use the Calinski and Harabasz (1974) index with method as k-means. Similar to Mclust, NbClust suggests a 2-cluster solution. Yet the proposed clusters in this solution are more equal in size, with 8 teams in Cluster 1 and 12 teams in Cluster 2.

Comparing the two packages' solutions, we see that Mclust classifies more teams in Cluster 1 than NbClust. 

```{r comparing packages}
table(mcl$classification,k.means$Best.partition)
```

Given that both packages suggested a two-cluster solution, we will carry out k-means clustering with 2 clusters. 

# K-means: 2 clusters

The analysis was run on all 12 variables in our data set. A seed was set for reproducibility purposes.

We also used a written function to calculate the *C(g)* for model comparison. The *C(g)* is equivalent to the Calinki & Harabasz (1974) criterion.

$C(g) = \frac{trace(Between)/g-1}{trace(Within)/n-g}$ where *g* is the number of clusters.

The goal is to maximize *C(g)*.

```{r kmeans 2 clusters}
set.seed(2011)
km.2 <- kmeans(EPL_Cluster[c(5:16)], 2, nstart= 100)

c.crit <- function(km.obj) {
  #based on k-means, for convenience due to amt of addl info in the km result object.
  #cd be generalized.
  sizes <- km.obj$size
  n <- sum(sizes)
  g <- length(sizes)
  msW<-sum(km.obj$withinss)/(n-g)
  overall.mean <- apply(km.obj$centers*km.obj$size,2,sum)/sum(km.obj$size)
  msB<-sum(km.obj$size*(t(t(km.obj$centers)-overall.mean))^2)/(g-1)
  list(msB=msB,msW=msW,C.g=msB/msW)
}
c.crit(km.2)$C.g
```

The *C(g)* for a 2-cluster solution is 36.595.

# K-means: 3 clusters

Although the results from Mclust and NbClust are trustworthy, it's safer to run an addtional k-means model to ensure that 2 clusters is the best solution given our data.

```{r kmeans 3 clusters}
set.seed(2011)
km.3 <- kmeans(EPL_Cluster[c(5:16)], 3, nstart= 100)
c.crit(km.3)$C.g
```

The *C(g)* for a 3-cluster solution is worse than that for a 2-cluster solution, which suggests that we should stick with only 2 clusters. 

# Exploring the 2-cluster Solution

## Silhouette Plot

First, we examine the silhouette plot of the clusters. The average silhouette width is 0.51, with Cluster 1 having a width of 0.47 and Cluster 2 having a width of 0.54. These widths suggest that our clusters are separated enough from each other such that the data points are well clustered. 

It is important to note that Everton (Team 6) has a rather small silhouette width. Since it's width is not negative, which would suggest a mismatch, we will continue on with the analysis. 

```{r silhouette widths, fig.width=6, fig.height=4}
plot(silhouette(km.2$clust, dist(EPL_Cluster[5:16])), main = "Silhouette Plot: 2 Cluster Solution")
```

## Plotting on Two Dimensions

Next, we will plot the clusters on two dimensions: Shots on Target and Shots on Target Conceded. These two dimensions were chosen since they appear to spearate the data best out of all the pairs plots.

```{r plot, fig.width = 6, fig.height=4}
#Separate by ShotsTargetSum and ShotsTargetConSum
plot(EPL_Cluster[,c(14,16)],col=km.2$clust,pch=1,cex=1.5, lwd=2, 
     xlab = "Shots on Target",ylab = "Shots on Target Conceded", 
     main = "2-Cluster Solution",ylim =c(100,260))
text(EPL_Cluster[,c(14,16)],labels=EPL_Cluster$Team,col=km.2$clust, cex=.5,pos=3)
```

Here we see two pretty well separated clusters. Most noteably, Cluster 2 contains the typical top4/6 teams -- Liverpool, Chelsea, Manchester City, Tottenham, Manchester United, Arsenal -- but also houses Southampton and Everton. 

# Comparison to Hierarchical Clustering

For sanity, we compare the two cluster k-means solution with a complete hierarchical clustering solution. The cluster dendrogram shows two clear clusters.

```{r hclust comparison}
plot(hclust(dist(EPL_Cluster[,-(1:4)]),meth='complete'))
comp.2 <- cutree(hclust(dist(EPL_Cluster[,-(1:4)]),meth='complete'),2)
```

When we compare the k-means results to the hierarchical clustering results, we see they match perfectly. 

```{r}
xtabs(~comp.2+km.2$cluster)
```

For procedure's sake, we also check the Rand Index between the two solutions. We find a perfect match. 

```{r}
RRand(km.2$clust, comp.2)
```

# Comparison to Multidimensional Scaling (MDS)

There are 12 possible dimensions which we can use to separate the 20 premier league teams from 16/17. In an effort to reduce the dimensionality, MDS with 2 dimensions was used. We end up with a similar plot to the k-means results, but with less separation between the two clusters. 

```{r MDS}
mds.2 <-cmdscale(dist(EPL_Cluster[,-(1:4)]),k=2)
pairs(mds.2)
plot(mds.2, ylim = c(-100,100), xlim = c(-250,275))
text(mds.2, labels = EPL_Cluster$Team, col = km.2$clust, cex=.5,pos=3)
```

This solution is not as satisfying as the k-means solution, but still suggests that there exists such a distance between the top4/6 teams and the rest. In particular, we see that relegated Sunderland and Hull are the farthest from the top4/6 teams on the first dimension. In a way, we can argue that the first dimension may primarily be attacking. Similarly, Manchester United and Arsenal are some distance away from Liverpool, Manchester City, Tottenham, and Chelsea, all of whom finished in the top 4. 

# Principal Components Analysis (PCA)

```{r PCA}
EPL_Cluster_std<- EPL_Cluster;
EPL_Cluster_std[,-(1:4)] <- scale(EPL_Cluster[,-(1:4)])
pca<- princomp(EPL_Cluster_std[,-(1:4)])
summary(pca)
```

It seems that 3 or 4 components should be enough to explain the variance in our data. 

## Screeplot

A screeplot was produced to determine an appropriate number of principal components.

```{r}
screeplot(pca,type="l", main = "Screeplot of PCA")
```

The "elbow" appears to be around 3. 

## Eigenvalues

Next, we examine the eigenvalues to solidify our decision to use 3 principal components. 

```{r}
eigen(cor(EPL_Cluster_std[,-(1:4)]))$values 
```

Only the first three components have an eigenvalue greater than 1, so we will use them in subsequent analysis. 

## Visualize the principal components

```{r}
plot(pca$scores[,1:2],col=1)
text(pca$scores[,1:2],col=1, labels=EPL_Cluster$Team,cex=.5)
#3D plot
plot3d(pca$scores[,1:3], cex=1.5)
text3d(pca$scores[,1:3], text=EPL_Cluster$Team, cex=.5)
```

2 components:

**Component 1: Attack versus Defense **

  -Main contributors: Shots Target, Shots Target Conceded, Shots, Shots Conceded, Goals, Goals        Conceded, Corners, Corners Conceded
  
**Component 2: Agressive play (cannot distinguish if aggressive play is attacking or defending)**
  
  -Main contributors: Yellow Card, Fouls Committed
  
**Component 3: ??? Mutual Agression**

  -Main contributors: Red Card, Fouls Against
  
**Component 4: ??? Dirty Play/Few Fouls Committed Against You**

  -Main contributors: Red Card, Fouls Against
  
# Revisiting MDS

The first three components have eigen values greater than 1, and also explain $\sim$ 83.70% of the variance, so we conduct MDS with three dimensions.

```{r MDS pt 2}
mds.3 <-cmdscale(dist(EPL_Cluster[,-(1:4)]),k=3)
pairs(mds.3)
plot(mds.3, ylim = c(-100,100), xlim = c(-250,275))
text(mds.3, labels = EPL_Cluster$Team, col = km.2$clust, cex=.5,pos=3)
```

# Clustering on PCA

Although there is controversy in clustering on PCA, we want to see if it better separates the teams.

## Two-Cluster Solution

Once again, we start with a two-cluster solution. 

```{r kmeans PCA}
set.seed(2011)
km.2pca <- kmeans(pca$scores, 2, nstart= 100)
c.crit(km.2pca)$C.g
```

The *C(g)* is 15.868.

## Three-Cluster Solution

For comparison, we run a three-cluster k-means model. 

```{r kmeans PCA 3 clusters}
set.seed(2011)
km.3pca <- kmeans(pca$scores, 3, nstart= 100)
c.crit(km.3pca)$C.g
```

Once again, three cluster solution is not an improvement, *C(g)* = 11.167. We will stick with our two clusters. 

## Exploring the Two-Cluster Solution

There are no differences in the teams assigned to each cluster across the two solutions. 

```{r compare km2 and km2pca}
xtabs(~km.2$cluster+km.2pca$cluster)
```


Below is a two-dimensional visualization of the two-cluster solution using PCAs. We see a similar separation to the k-means solution on the raw data in that the top 4/6 teams score higher on Component 1. 

```{r}
plot(pca$scores[,1:2],col=km.2pca$clust,pch=1,cex=1.5, lwd=2, 
   main = "2-Cluster Solution on PCA", ylim = c(-3,3))
text(pca$scores[,1:2],labels=EPL_Cluster$Team,col=km.2pca$clust, cex=.5,pos=3)
```

## 3D Plot

Since 3D plots are cool, here are the 3 principal components colored by their respective k-means clusters. 

```{r}
plot3d(pca$scores)
text3d(pca$scores, text = EPL_Cluster$Team, col = km.2pca$clust, cex=.5,pos=3)
```

