---
title: "K-means Clustering of Premier League Teams 16/17"
author: "Clare Clingain"
date: "May 1, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
require(mclust)
library(NbClust)
require(phyclust)
library(cluster)
EPL_aggregate <- read.csv("Clean Data/EPL_aggregate.csv")
```

Sports analysts and fans alike try to decipher what sets England's top teams apart from each other. What makes the elusive top 4/6 so far away from the rest? What about the relegation-threatened teams? Using k-means unsupervised learning, we will attempt to cluster teams based on season-total statistics from the 2016-2017 season to determine whether there are ways in which we can separate the top-performing teams from the rest of the pack. Similarly, we will try to see which teams are squeezing their way into the top 4/6. 

# Data Preparation

Using publicly available data we cleaned and manipulated, we created a new data set *EPL_Aggregate*, which can be found [here](https://github.com/CClingain/EPL_Data/tree/master/Clean%20Data), that contains the summary statistics for the 20 Premier League teams from 2000/2001 to 2017/2018 seasons. Twelve different stats were recorded over the course of each season for each team: Yellow Cards, Red Cards, Goals Scored, Goals Against, Fouls Committed, Fouls Against, Corners Won, Corners Conceded, Shots, Shots Conceded, Shots on Target, Shots on Target Conceded. For this analysis, we summed each of the twelve stats for each team to get an idea of the "total" performance of a team over the course of the  2016/2017 season.

**Teams for 16/17:** Arsenal, Bournemouth, Burnley, Chelsea, Crystal Palace, Everton, Hull, Leicester City, Liverpool, Manchester City, Manchester United, Middlesbrough, Southampton, Stoke, Sunderland, Swansea, Tottenham, Watford, West Bromwich Albion, West Ham United.

```{r Data Preparation}
EPL_Cluster <- EPL_aggregate %>% filter(Season=="16/17") %>% group_by(Team) %>%
  mutate(YellowCardSum = sum(Yellow_Cards))

EPL_Cluster <- EPL_Cluster %>% mutate(RedCardSum = sum(Red_Cards)) %>% 
  mutate(GoalsSum = sum(Goals_Scored)) %>%
  mutate(GoalsAgainstSum = sum(Goals_Conceded)) %>% 
  mutate(FoulsSum = sum(Fouls_Committed)) %>% 
  mutate(FoulsAgainstSum = sum(Fouls_Against)) %>%
  mutate(CornersSum = sum(Corners)) %>% 
  mutate(CornersConcededSum = sum(Corners_Conceded)) %>% 
  mutate(ShotsSum = sum(Shots_Taken)) %>%
  mutate(ShotsTargetSum = sum(Shots_on_Target)) %>%
  mutate(ShotsCondededSum = sum(Shots_Conceded)) %>% 
  mutate(ShotsTargetConSum = sum(Shots_on_Target_Conceded))
EPL_Cluster <- EPL_Cluster %>% filter(Week==38)
EPL_Cluster <- EPL_Cluster[c(1:4, 31:42)]
```

# How many clusters?

There are a couple packages out there that do a good job of determining the number of clusters that might exist in a given data set. The Mclust package provides BIC, classification, uncertainty, and density for model-based clustering using an EM algorithm. 

```{r clusters suggestion}
mcl <- Mclust(EPL_Cluster[c(5:16)])
summary(mcl)
```

The results from Mclust reveal that the best model for our data is a 2-cluster VEV solution. The suggested clustering split has 15 teams in Cluster 1 and 5 teams in Cluster 2. However, this portion of the results from Mclust should be taken with caution. K-means may very well cluster teams in a different way.

```{r NbClust}
k.means <- NbClust(EPL_Cluster[c(5:16)],method='kmeans',index='ch')
k.means$Best.nc
table(k.means$Best.partition)
```

NbClust provides a similar analysis for determining the optimal number of clusters in a data set. The maximum index is selected. We chose to use the Calinski and Harabasz (1974) index with method as k-means. Similar to Mclust, NbClust suggests a 2-cluster solution. Yet the proposed clusters in this solution are more equal in size, with 8 teams in Cluster 1 and 12 teams in Cluster 2.

Comparing the two packages' solutions, we see that Mclust classifies more teams in Cluster 1 than NbClust. 

```{r comparing packages}
table(mcl$classification,k.means$Best.partition)
```

Given that both packages suggested a two-cluster solution, we will carry out k-means clustering with 2 clusters. 

# K-means: 2 clusters

The analysis was run on all 12 variables in our data set. A seed was set for reproducibility purposes.

We also used a written function to calculate the *C(g)* for model comparison. The *C(g)* is equivalent to the Calinki & Harabasz (1974) criterion.

$C(g) = \frac{trace(Between)/g-1}{trace(Within)/n-g}$ where *g* is the number of clusters.

The goal is to maximize *C(g)*.

```{r kmeans 2 clusters}
set.seed(2011)
km.2 <- kmeans(EPL_Cluster[c(5:16)], 2, nstart= 100)

c.crit <- function(km.obj) {
  #based on k-means, for convenience due to amt of addl info in the km result object.
  #cd be generalized.
  sizes <- km.obj$size
  n <- sum(sizes)
  g <- length(sizes)
  msW<-sum(km.obj$withinss)/(n-g)
  overall.mean <- apply(km.obj$centers*km.obj$size,2,sum)/sum(km.obj$size)
  msB<-sum(km.obj$size*(t(t(km.obj$centers)-overall.mean))^2)/(g-1)
  list(msB=msB,msW=msW,C.g=msB/msW)
}
c.crit(km.2)$C.g
```

The *C(g)* for a 2-cluster solution is 36.595.

# K-means: 3 clusters

Although the results from Mclust and NbClust are trustworthy, it's safer to run an addtional k-means model to ensure that 2 clusters is the best solution given our data.

```{r kmeans 3 clusters}
set.seed(2011)
km.3 <- kmeans(EPL_Cluster[c(5:16)], 3, nstart= 100)
c.crit(km.3)$C.g
```

The *C(g)* for a 3-cluster solution is worse than that for a 2-cluster solution, which suggests that we should stick with only 2 clusters. 

# Exploring the 2-cluster Solution

First, we examine the silhouette plot of the clusters. The average silhouette width is 0.51, with Cluster 1 having a width of 0.47 and Cluster 2 having a width of 0.54. These widths suggest that our clusters are separated enough from each other such that the data points are well clustered. 

It is important to note that Everton (Team 6) has a rather small silhouette width. Since it's width is not negative, which would suggest a mismatch, we will continue on with the analysis. 

```{r silhouette widths, fig.width=6, fig.height=4}
plot(silhouette(km.2$clust, dist(EPL_Cluster[5:16])), main = "Silhouette Plot: 2 Cluster Solution")
```

Next, we will plot the clusters on two dimensions: Shots on Target and Shots on Target Conceded. These two dimensions were chosen since they appear to spearate the data best out of all the pairs plots.

```{r plot, fig.width = 6, fig.height=4}
#Separate by ShotsTargetSum and ShotsTargetConSum
plot(EPL_Cluster[,c(14,16)],col=km.2$clust,pch=1,cex=1.5, lwd=2, 
     xlab = "Shots on Target",ylab = "Shots on Target Conceded", 
     main = "2-Cluster Solution",ylim =c(100,260))
text(EPL_Cluster[,c(14,16)],labels=EPL_Cluster$Team,col=km.2$clust, cex=.5,pos=3)
```

Here we see two pretty well separated clusters. Most noteably, Cluster 2 contains the typical top4/6 teams -- Liverpool, Chelsea, Manchester City, Tottenham, Manchester United, Arsenal -- but also houses Southampton and Everton. 

# Comparison to Hierarchical Clustering

For sanity, we compare the two cluster k-means solution with a complete hierarchical clustering solution. The cluster dendrogram shows two clear clusters.

```{r hclust comparison}
plot(hclust(dist(EPL_Cluster[,-(1:4)]),meth='complete'))
comp.2 <- cutree(hclust(dist(EPL_Cluster[,-(1:4)]),meth='complete'),2)
```

When we compare the k-means results to the hierarchical clustering results, we see they match perfectly. 

```{r}
xtabs(~comp.2+km.2$cluster)
```

For procedure's sake, we also check the Rand Index between the two solutions. We find a perfect match. 

```{r}
RRand(km.2$clust, comp.2)
```

# Comparison to Multidimensional Scaling (MDS)

There are 12 possible dimensions which we can use to separate the 20 premier league teams from 16/17. In an effort to reduce the dimensionality, MDS with 2 dimensions was used. We end up with a similar plot to the k-means results, but with less separation between the two clusters. 

```{r MDS}
mds.2 <-cmdscale(dist(EPL_Cluster[,-(1:4)]),k=2)
pairs(mds.2)
plot(mds.2)
text(mds.2, labels = EPL_Cluster$Team, col = km.2$clust, cex=.5,pos=3)
```

This solution is not as satisfying as the k-means solution, but still suggests that there exists such a distance between the top4/6 teams and the rest. In particular, we see that relegated Sunderland and Hull are the farthest from the top4/6 teams on the first dimension. 